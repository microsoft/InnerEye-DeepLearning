<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Changelog" href="CHANGELOG.html" /><link rel="prev" title="Releases" href="releases.html" />

    <meta name="generator" content="sphinx-5.0.2, furo 2022.06.21"/>
        <title>Training of self-supervised models - InnerEye-DeepLearning 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?digest=40978830699223671f4072448e654b5958f38b89" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">InnerEye-DeepLearning 1.0.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">InnerEye-DeepLearning 1.0.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="innereye_deeplearning.html">InnerEye-DeepLearning</a></li>
<li class="toctree-l1"><a class="reference internal" href="WSL.html">How to use the Windows Subsystem for Linux (WSL2) for development</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment.html">Set up InnerEye-DeepLearning</a></li>
<li class="toctree-l1"><a class="reference internal" href="setting_up_aml.html">How to setup Azure Machine Learning for InnerEye</a></li>
<li class="toctree-l1"><a class="reference internal" href="creating_dataset.html">Dataset Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="building_models.html">Building Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_tasks.html">Sample Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="debugging_and_monitoring.html">Debugging and Monitoring Jobs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Further reading for contributors</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pull_requests.html">Suggested Workflow for Pull Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Pytest and testing on CPU and GPU machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hello_world_model.html">Training a Hello World segmentation model</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_on_aml.html">Model Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="bring_your_own_model.html">Bring Your Own PyTorch Lightning Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="fastmri.html">Working with FastMRI models</a></li>
<li class="toctree-l1"><a class="reference internal" href="innereye_as_submodule.html">Using the InnerEye code as a git submodule of your project</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_diagnostics.html">Model Diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="move_model.html">Move a model to other workspace</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases.html">Releases</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Training of self-supervised models</a></li>
<li class="toctree-l1"><a class="reference internal" href="CHANGELOG.html">Changelog</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API documentation (🚧 Work In Progress 🚧)</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../rst/api/ML/index.html">Machine learning</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/configs.html">Segmentation Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/runner.html">Runner</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/augmentations.html">Data augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/photometric_normalization.html">Photometric normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/pipelines.html">Pipelines</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="training-of-self-supervised-models">
<h1>Training of self-supervised models<a class="headerlink" href="#training-of-self-supervised-models" title="Permalink to this heading"></a></h1>
<p>The code present in the <a class="reference external" href="https://github.com/microsoft/InnerEye-DeepLearning/tree/main/InnerEye/ML/SSL">InnerEye/ML/SSL</a>
folder allows you to train self-supervised models using
<a class="reference external" href="http://proceedings.mlr.press/v119/chen20j/chen20j.pdf">SimCLR</a> or
<a class="reference external" href="https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf">BYOL</a>. This code runs as a “
bring-your-own-model” self-contained module (
see the [bring-your-own-model instructions]](bring_your_own_model.md)
.</p>
<p>Here, we provide implementations for four datasets to get you kickstarted with self-supervised models:</p>
<ul class="simple">
<li><p>Toy CIFAR datasets: <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a>
and <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR100</a></p></li>
<li><p>Medical Chest-Xray
datasets: <a class="reference external" href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/overview">RSNA Pneumonia Detection Challenge</a>
data (30k scans, labels indicating presence of pneumonia),
<a class="reference external" href="https://www.kaggle.com/nih-chest-xrays/data">NIH Chest-Xray</a> (112k Chest-Xray scans) or
<a class="reference external" href="https://stanfordmlgroup.github.io/competitions/chexpert/">CheXpert</a> (228k scans).</p></li>
</ul>
<section id="multi-dataset-support">
<h2>Multi-dataset support<a class="headerlink" href="#multi-dataset-support" title="Permalink to this heading"></a></h2>
<p>During self-supervised training, a separate linear classifier is trained on top of learnt image embeddings. In this way,
users can continuously monitor the representativeness of learnt image embeddings for a given downstream classification
task. More importantly, the framework allows users to specify multiple datasets and data loaders for SimCLR/BYOL
training and evaluation. For instance, a BYOL encoder can be learnt using a dataset that does not contain any target
labels and embeddings can be evaluated throughout training on a separate dataset containing class labels. To enable this
functionality, our SSLContainer takes two dataset names parameters: <code class="docutils literal notranslate"><span class="pre">ssl_training_dataset_name</span></code> to indicate which
dataset to use for SSL training and <code class="docutils literal notranslate"><span class="pre">linear_head_dataset_name</span></code> to indicate which dataset for the classification task
used to monitor embeddings quality during training.</p>
</section>
<section id="quick-start-guide">
<h2>Quick start guide<a class="headerlink" href="#quick-start-guide" title="Permalink to this heading"></a></h2>
<p>Here we described how to quickly start a training job with our ready made configs.</p>
<section id="example-1-training-a-simclr-or-byol-model-on-cifar10">
<h3>Example 1: training a SimCLR or BYOL model on CIFAR10<a class="headerlink" href="#example-1-training-a-simclr-or-byol-model-on-cifar10" title="Permalink to this heading"></a></h3>
<p>To kick-off a training for a SimCLR and BYOL models on CIFAR10, simply run</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python ML/runner.py --model<span class="o">=</span>CIFAR10BYOL
python ML/runner.py --model<span class="o">=</span>CIFAR10SimCLR
</pre></div>
</div>
<p>For this dataset, it will automatically take care of downloading the dataset to your machine prior to starting training.</p>
</section>
<section id="example-2-training-a-byol-model-on-chest-xray-data">
<h3>Example 2: training a BYOL model on Chest-Xray data<a class="headerlink" href="#example-2-training-a-byol-model-on-chest-xray-data" title="Permalink to this heading"></a></h3>
<section id="step-0-get-the-data">
<h4>Step 0: Get the data<a class="headerlink" href="#step-0-get-the-data" title="Permalink to this heading"></a></h4>
</section>
<section id="if-you-run-on-your-local-machine">
<h4>If you run on your local machine<a class="headerlink" href="#if-you-run-on-your-local-machine" title="Permalink to this heading"></a></h4>
<p>Prior to starting training a model on this dataset, you will need to download it from Kaggle to your machine:</p>
<ul class="simple">
<li><p>To use the RSNA Pneumonia Detection Challenge data: please download from
<a class="reference external" href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data?select=stage_2_train_images">here</a>. Make sure to
download all images and the <code class="docutils literal notranslate"><span class="pre">dataset.csv</span></code> file to your data folder. Please note that the labels are here merely used
for monitoring purposes.</p></li>
<li><p>To get the NIH dataset: please download from <a class="reference external" href="https://www.kaggle.com/nih-chest-xrays/data">here</a>. Make sure you
include also the csv files in your data folder, the code assumes the dataset files and all the images lie in your data
folder as downloaded from Kaggle. In particular, do not modify the original csv filenames (e.g. the code expects to
find the <code class="docutils literal notranslate"><span class="pre">Data_Entry_2017.csv</span></code> file within the data directory).</p></li>
<li><p>To get the CheXpert data: please download from <a class="reference external" href="https://stanfordmlgroup.github.io/competitions/chexpert/">here</a>, the
code assumes the dataset files and all the images lie in your data folder as downloaded.</p></li>
</ul>
</section>
<section id="if-you-run-on-aml">
<h4>If you run on AML<a class="headerlink" href="#if-you-run-on-aml" title="Permalink to this heading"></a></h4>
<p>In order to train models on AML you will need to upload the datasets listed above to your storage account and get their
dataset_id to pass to your model config.</p>
</section>
<section id="step-1-update-your-model-config">
<h4>Step 1: Update your model config<a class="headerlink" href="#step-1-update-your-model-config" title="Permalink to this heading"></a></h4>
<p>We provide sample configs to train models on NIH data both for BYOL and SimCLR. You can use them as they are except for
the dataset location fields:</p>
<ul class="simple">
<li><p>If you’re running locally set the <code class="docutils literal notranslate"><span class="pre">local_dataset</span></code> parameter to point to your data folder.</p></li>
<li><p>If you’re running on AML: you need to update the <code class="docutils literal notranslate"><span class="pre">RSNA_AZURE_DATASET_ID</span></code> and <code class="docutils literal notranslate"><span class="pre">NIH_AZURE_DATASET_ID</span></code> variables to point
to the name of the NIH and Kaggle dataset in your own workspace.</p></li>
</ul>
</section>
<section id="step-2-launch-the-training-job">
<h4>Step 2: Launch the training job<a class="headerlink" href="#step-2-launch-the-training-job" title="Permalink to this heading"></a></h4>
<p>Example to train a SSL model with BYOL on the NIH dataset and monitor the embeddings quality on the Kaggle RSNA
Pneumonia Challenge classification task:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python ML/runner.py --model<span class="o">=</span>NIH_RSNA_BYOL
</pre></div>
</div>
</section>
</section>
</section>
<section id="configuring-your-own-ssl-models">
<h2>Configuring your own SSL models<a class="headerlink" href="#configuring-your-own-ssl-models" title="Permalink to this heading"></a></h2>
<section id="about-sslcontainer-configuration">
<h3>About SSLContainer configuration<a class="headerlink" href="#about-sslcontainer-configuration" title="Permalink to this heading"></a></h3>
<p>All SSL models are derived from
the <a class="reference external" href="https://github.com/microsoft/InnerEye-DeepLearning/blob/main/InnerEye/ML/SSL/lightning_containers/ssl_container.py">SSLcontainer</a>
class. See the config class in <a class="reference external" href="https://github.com/microsoft/InnerEye-DeepLearning/tree/main/InnerEye/ML/configs/ssl">ML/configs/ssl</a> for some examples of specific model configurations (all derived
from this container).</p>
<p>If you wish to create your own model config for SSL training, you will need to create a child class and parametrize it
with the following available arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ssl_training_dataset_name</span></code>: the name of the dataset to train the SSL encoder on, a member of the SSLDatasetName
class (don’t forget to update this class if you’re adding a new dataset ;)),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">linear_head_dataset_name</span></code>: the name of the dataset to train to linear head on top of the classifier for monitoring
purposes,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">azure_dataset_id</span></code>: the id of the AML dataset to use for SSL training,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">extra_azure_dataset_ids</span></code>: dataset_id to use for linear head training, expected to be provided as a list [data-id],</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ssl_encoder</span></code>: name of the encoder to train, member of <code class="docutils literal notranslate"><span class="pre">EncoderName</span></code> class, currently supported are resnet50,
resnet101 and densenet121,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ssl_training_type</span></code>: which SSL algorithm to use, member of <code class="docutils literal notranslate"><span class="pre">SSLType</span></code> choice between BYOL and SimCLR,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ssl_training_batch_size</span></code>: batch size of SSL training. This is the number of examples processed by a single GPU.
Multiply this by the number of GPUs to get the effective batch size.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">linear_head_batch_size</span></code>: batch size for linear head training (used for monitor of SSL embeddings quality). This is
the number of examples processed by a single GPU. Multiply this by the number of GPUs to get the effective batch size.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ssl_augmentation_config</span></code>: path to yaml config for augmentation to use during SSL training. Only used for NIH/Kaggle
datasets.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">linear_head_augmentation_config</span></code>: path to yaml config for augmentation to use for linear head training. Only used for
NIH/Kaggle datasets,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_balanced_binary_loss_for_linear_head</span></code>: whether to use balanced loss for linear head training,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_seed</span></code>: seed for the run,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_epochs</span></code>: number of epochs to train for.</p></li>
</ul>
<p>In case you wish to first test your model locally, here some optional arguments that can be useful:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">local_dataset</span></code>: path to local dataset, if passed the azure dataset will be ignored</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_debug_model</span></code>: if True it will only run on the first batch of each epoch</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">drop_last</span></code>: if False (True by default) it will keep the last batch also if incomplete</p></li>
</ul>
</section>
<section id="creating-your-own-datamodules">
<h3>Creating your own datamodules<a class="headerlink" href="#creating-your-own-datamodules" title="Permalink to this heading"></a></h3>
<p>To use this code with your own data, you will need to:</p>
<ol class="simple">
<li><p>Define your own Lightening Container that inherits from <code class="docutils literal notranslate"><span class="pre">SSLContainer</span></code> as described in the paragraph above.</p></li>
<li><p>Create a dataset class that reads your new dataset, inheriting from both <code class="docutils literal notranslate"><span class="pre">VisionDataset</span></code>
and <code class="docutils literal notranslate"><span class="pre">InnerEyeDataClassBaseWithReturnIndex</span></code>. See for example how we constructed <code class="docutils literal notranslate"><span class="pre">RSNAKaggleCXR</span></code>
class. WARNING: the first positional argument of your dataset class constructor MUST be the data directory (”root”),
as VisionDataModule expects this in the prepare_data step.</p></li>
<li><p>In your own container update the <code class="docutils literal notranslate"><span class="pre">_SSLDataClassMappings</span></code> member of the class so that the code knows which data class
to associate to your new dataset name.</p></li>
<li><p>Create a yaml configuration file that contains the augmentations specific to your dataset. The yaml file will be
consumed by the <code class="docutils literal notranslate"><span class="pre">create_transforms_from_config</span></code> function defined in the
<code class="docutils literal notranslate"><span class="pre">InnerEye.ML.augmentations.transform_pipeline</span></code> module (see next paragraph for more details). Alternatively, overwrite
the <code class="docutils literal notranslate"><span class="pre">_get_transforms</span></code> method. To simplify this step, we have defined a series of standard operations in
<code class="docutils literal notranslate"><span class="pre">SSL/transforms_utils.py</span></code> . You could for example construct a transform pipeline similar to the one created
inside <code class="docutils literal notranslate"><span class="pre">create_transform_from_config</span></code> inside your own method.</p></li>
<li><p>Update all necessary parameters in the model config (cf. previous paragraph)</p></li>
</ol>
<p>Once all these steps are updated, the code in the base SSLContainer class will take care of creating the corresponding
datamodules for SSL training and linear head monitoring.</p>
</section>
<section id="about-the-augmentation-configuration-yaml-file">
<h3>About the augmentation configuration yaml file<a class="headerlink" href="#about-the-augmentation-configuration-yaml-file" title="Permalink to this heading"></a></h3>
<p>The augmentations used for SSL training for all Chest-X-rays models are parametrized via a yaml config file. The path to
this config as to be passed in the model config (cf. section above). We provide two defaults
configs: <code class="docutils literal notranslate"><span class="pre">cxr_ssl_encoder_augmentations.yaml</span></code> is used to define the augmentations used for BYOL/SimCLR training;
the <code class="docutils literal notranslate"><span class="pre">cxr_linear_head_augmentations.yaml</span></code> config defines the augmentations to used for the training of the linear head
(used for monitoring purposes). The meaning of each config argument is detailed in <code class="docutils literal notranslate"><span class="pre">ssl_model_config.py</span></code></p>
<p>WARNING: this file will be ignored for CIFAR examples where we use the default pl-bolts augmentations.</p>
</section>
</section>
<section id="finetuning-a-linear-head-on-top-of-a-pretrained-ssl-model">
<h2>Finetuning a linear head on top of a pretrained SSL model<a class="headerlink" href="#finetuning-a-linear-head-on-top-of-a-pretrained-ssl-model" title="Permalink to this heading"></a></h2>
<p>Alongside with the modules to train your SSL models, we also provide examplary modules that allow you to build a
classifier on top of a pretrained SSL model. The base class for these modules is <code class="docutils literal notranslate"><span class="pre">SSLClassifierContainer</span></code>. It builds on
top of the <code class="docutils literal notranslate"><span class="pre">SSLContainer</span></code> with additional command line arguments allowing you to specify where to find the checkpoint
for your pretrained model. For this you have two options:</p>
<ul class="simple">
<li><p>If you are running locally, you can provide the local path to your pretrained model checkpoint
via <code class="docutils literal notranslate"><span class="pre">--local_weights_path</span></code>.</p></li>
<li><p>If your are running on AML, use the <code class="docutils literal notranslate"><span class="pre">pretraining_run_recovery_id</span></code> field. Providing this field, will mean that AML will
automatically download the checkpoints to the current node, will pick the latest checkpoint to build the classifier on
top. Beware not to confuse <code class="docutils literal notranslate"><span class="pre">pretraining_run_recovery_id</span></code> with <code class="docutils literal notranslate"><span class="pre">run_recovery_id</span></code> as the latter is use to continue training on
the same model (which is not the case here).</p></li>
</ul>
<p>The code will then automatically extract the encoder part of the loaded SSL model to initialize your classifier. You can
then also choose whether you want to freeze the weights of your encoder or not via the <code class="docutils literal notranslate"><span class="pre">--freeze_encoder=True/False</span></code>
argument. By default, this is set to True.</p>
<section id="example-for-cifar">
<h3>Example for CIFAR<a class="headerlink" href="#example-for-cifar" title="Permalink to this heading"></a></h3>
<p>We provide an example of such a classifier container for CIFAR named <code class="docutils literal notranslate"><span class="pre">SSLClassifierCIFAR</span></code>. To launch a finetuning run
for this model on CIFAR10, just run</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python ML/runner.py --model<span class="o">=</span>SSLClassifierCIFAR --pretraining_run_recovery_id<span class="o">={</span>THE_ID_TO_YOUR_SSL_TRAINING_JOB<span class="o">}</span>
</pre></div>
</div>
</section>
<section id="example-for-cxr">
<h3>Example for CXR<a class="headerlink" href="#example-for-cxr" title="Permalink to this heading"></a></h3>
<p>Similarly, we provide class to allow you to simply start a finetuning job for CXR model in <code class="docutils literal notranslate"><span class="pre">CXRImageClassifier</span></code>. By
default, this will launch a finetuning job on the RSNA Pneumonia dataset. To start the run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python ML/runner.py --model<span class="o">=</span>CXRImageClassifier --pretraining_run_recovery_id<span class="o">={</span>THE_ID_TO_YOUR_SSL_TRAINING_JOB<span class="o">}</span>
</pre></div>
</div>
<p>or for a local run</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python ML/runner.py --model<span class="o">=</span>CXRImageClassifier --local_weights_path<span class="o">={</span>LOCAL_PATH_TO_YOUR_SSL_CHECKPOINT<span class="o">}</span>
</pre></div>
</div>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="CHANGELOG.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Changelog</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="releases.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Releases</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; Microsoft Corporation
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Training of self-supervised models</a><ul>
<li><a class="reference internal" href="#multi-dataset-support">Multi-dataset support</a></li>
<li><a class="reference internal" href="#quick-start-guide">Quick start guide</a><ul>
<li><a class="reference internal" href="#example-1-training-a-simclr-or-byol-model-on-cifar10">Example 1: training a SimCLR or BYOL model on CIFAR10</a></li>
<li><a class="reference internal" href="#example-2-training-a-byol-model-on-chest-xray-data">Example 2: training a BYOL model on Chest-Xray data</a><ul>
<li><a class="reference internal" href="#step-0-get-the-data">Step 0: Get the data</a></li>
<li><a class="reference internal" href="#if-you-run-on-your-local-machine">If you run on your local machine</a></li>
<li><a class="reference internal" href="#if-you-run-on-aml">If you run on AML</a></li>
<li><a class="reference internal" href="#step-1-update-your-model-config">Step 1: Update your model config</a></li>
<li><a class="reference internal" href="#step-2-launch-the-training-job">Step 2: Launch the training job</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#configuring-your-own-ssl-models">Configuring your own SSL models</a><ul>
<li><a class="reference internal" href="#about-sslcontainer-configuration">About SSLContainer configuration</a></li>
<li><a class="reference internal" href="#creating-your-own-datamodules">Creating your own datamodules</a></li>
<li><a class="reference internal" href="#about-the-augmentation-configuration-yaml-file">About the augmentation configuration yaml file</a></li>
</ul>
</li>
<li><a class="reference internal" href="#finetuning-a-linear-head-on-top-of-a-pretrained-ssl-model">Finetuning a linear head on top of a pretrained SSL model</a><ul>
<li><a class="reference internal" href="#example-for-cifar">Example for CIFAR</a></li>
<li><a class="reference internal" href="#example-for-cxr">Example for CXR</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/scripts/furo.js"></script>
    </body>
</html>