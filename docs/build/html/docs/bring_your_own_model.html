<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Working with FastMRI models" href="fastmri.html" /><link rel="prev" title="Model Deployment" href="deploy_on_aml.html" />

    <meta name="generator" content="sphinx-5.0.2, furo 2022.06.21"/>
        <title>Bring Your Own PyTorch Lightning Model - InnerEye-DeepLearning 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?digest=40978830699223671f4072448e654b5958f38b89" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">InnerEye-DeepLearning 1.0.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">InnerEye-DeepLearning 1.0.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="innereye_deeplearning.html">InnerEye-DeepLearning</a></li>
<li class="toctree-l1"><a class="reference internal" href="WSL.html">How to use the Windows Subsystem for Linux (WSL2) for development</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment.html">Set up InnerEye-DeepLearning</a></li>
<li class="toctree-l1"><a class="reference internal" href="setting_up_aml.html">How to setup Azure Machine Learning for InnerEye</a></li>
<li class="toctree-l1"><a class="reference internal" href="creating_dataset.html">Dataset Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="building_models.html">Building Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_tasks.html">Sample Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="debugging_and_monitoring.html">Debugging and Monitoring Jobs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Further reading for contributors</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pull_requests.html">Suggested Workflow for Pull Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Pytest and testing on CPU and GPU machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hello_world_model.html">Training a Hello World segmentation model</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_on_aml.html">Model Deployment</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Bring Your Own PyTorch Lightning Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="fastmri.html">Working with FastMRI models</a></li>
<li class="toctree-l1"><a class="reference internal" href="innereye_as_submodule.html">Using the InnerEye code as a git submodule of your project</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_diagnostics.html">Model Diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="move_model.html">Move a model to other workspace</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="self_supervised_models.html">Training of self-supervised models</a></li>
<li class="toctree-l1"><a class="reference internal" href="CHANGELOG.html">Changelog</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API documentation (üöß Work In Progress üöß)</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../rst/api/ML/index.html">Machine learning</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/configs.html">Segmentation Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/runner.html">Runner</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/augmentations.html">Data augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/photometric_normalization.html">Photometric normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/pipelines.html">Pipelines</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="bring-your-own-pytorch-lightning-model">
<h1>Bring Your Own PyTorch Lightning Model<a class="headerlink" href="#bring-your-own-pytorch-lightning-model" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>The InnerEye toolbox is capable of training any PyTorch Lighting (PL) model inside of AzureML, making
use of all the usual InnerEye toolbox features:</p>
<ul class="simple">
<li><p>Working with different model in the same codebase, and selecting one by name</p></li>
<li><p>Distributed training in AzureML</p></li>
<li><p>Logging via AzureML‚Äôs native capabilities</p></li>
<li><p>Training on a local GPU machine or inside of AzureML without code changes</p></li>
<li><p>Supply commandline overrides for model configuration elements, to quickly queue many jobs</p></li>
</ul>
<p>This can be used by</p>
<ul class="simple">
<li><p>Defining a special container class, that encapsulates the PyTorch Lighting model to train, and the data that should
be used for training and testing.</p></li>
<li><p>Adding essential trainer parameters like number of epochs to that container.</p></li>
<li><p>Invoking the InnerEye runner and providing the name of the container class, like this:
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">InnerEye/ML/runner.py</span> <span class="pre">--model=MyContainer</span></code>. To train in AzureML, just add a <code class="docutils literal notranslate"><span class="pre">--azureml</span></code> flag.</p></li>
</ul>
<p>There is a fully working example <a class="reference external" href="InnerEye/ML/configs/other/HelloContainer.py">HelloContainer</a> that implements
a simple 1-dimensional regression model from data stored in a CSV file. You can run that
from the command line by <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">InnerEye/ML/runner.py</span> <span class="pre">--model=HelloContainer</span></code>.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>In order to use these capabilities, you need to implement a class deriving from <code class="docutils literal notranslate"><span class="pre">LightningContainer</span></code>. This class
encapsulates everything that is needed for training with PyTorch Lightning:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">create_model</span></code> method needs to return a subclass of <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>, that has
all the usual PyTorch Lightning methods required for training, like the <code class="docutils literal notranslate"><span class="pre">training_step</span></code> and <code class="docutils literal notranslate"><span class="pre">forward</span></code> methods. This
object needs to adhere to additional constraints, see below.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">get_data_module</span></code> method of the container needs to return a <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> that has the data loaders for
training and validation data.</p></li>
<li><p>The optional <code class="docutils literal notranslate"><span class="pre">get_inference_data_module</span></code> returns a <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> that is used to read the data for inference
(that is, evaluating the trained model). By default, this returns the same data as <code class="docutils literal notranslate"><span class="pre">get_training_data_module</span></code>, but you
can override this for special models like segmentation models that are trained on equal sized image patches, but
evaluated on full images of varying size.</p></li>
</ul>
<p>Your class needs to be defined in a Python file in the <code class="docutils literal notranslate"><span class="pre">InnerEye/ML/configs</span></code> folder, otherwise it won‚Äôt be picked up
correctly. If you‚Äôd like to have your model defined in a different folder, please specify the Python namespace via
the <code class="docutils literal notranslate"><span class="pre">--model_configs_namespace</span></code> argument. For example, use <code class="docutils literal notranslate"><span class="pre">--model_configs_namespace=My.Own.configs</span></code> if your
model configuration classes reside in folder <code class="docutils literal notranslate"><span class="pre">My/Own/configs</span></code> from the repository root.</p>
<section id="cross-validation">
<h3>Cross Validation<a class="headerlink" href="#cross-validation" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>If you are doing cross validation you need to ensure that the <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> returned by your container‚Äôs
<code class="docutils literal notranslate"><span class="pre">get_data_module</span></code> method:</p>
<ul class="simple">
<li><p>Needs to take into account the number of cross validation splits, and the cross validation split index when
preparing the data.</p></li>
<li><p>Needs to log val/Loss in its <code class="docutils literal notranslate"><span class="pre">validation_step</span></code> method.
You can find a working example of handling cross validation in the
<a class="reference external" href="https://github.com/microsoft/InnerEye-DeepLearning/tree/main/InnerEye/ML/configs/other/HelloContainer.py">HelloContainer</a> class.</p></li>
</ul>
<p><em>Example</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">LightningModule</span><span class="p">,</span> <span class="n">LightningDataModule</span>
<span class="kn">from</span> <span class="nn">InnerEye.ML.lightning_container</span> <span class="kn">import</span> <span class="n">LightningContainer</span>

<span class="k">class</span> <span class="nc">MyLightningModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="o">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="o">...</span>
    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">...</span>
    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="o">...</span>

<span class="k">class</span> <span class="nc">MyDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">root_path</span><span class="p">:</span> <span class="n">Path</span><span class="p">):</span>
        <span class="c1"># All data should be read from the folder given in self.root_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">root_path</span> <span class="o">=</span> <span class="n">root_path</span>
    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
        <span class="c1"># The data should be read off self.root_path</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="o">...</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
        <span class="c1"># The data should be read off self.root_path</span>
        <span class="n">val_dataset</span> <span class="o">=</span> <span class="o">...</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
        <span class="c1"># The data should be read off self.root_path</span>
        <span class="n">test_dataset</span> <span class="o">=</span> <span class="o">...</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MyContainer</span><span class="p">(</span><span class="n">LightningContainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">azure_dataset_id</span> <span class="o">=</span> <span class="s2">&quot;folder_name_in_azure_blob_storage&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_dataset</span> <span class="o">=</span> <span class="s2">&quot;/some/local/path&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">42</span>

    <span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LightningModule</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">MyLightningModel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_data_module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LightningDataModule</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">MyDataModule</span><span class="p">(</span><span class="n">root_path</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>Where does the data for training come from?</p>
<ul class="simple">
<li><p>When training a model on a local box or VM, the data is read from the <code class="docutils literal notranslate"><span class="pre">local_dataset</span></code> folder that you define in the
container.</p></li>
<li><p>When training a model in AzureML, the code searches for a folder called <code class="docutils literal notranslate"><span class="pre">folder_name_in_azure_blob_storage</span></code> in
Azure blob storage. That is then downloaded or mounted. The local download path is then copied over the <code class="docutils literal notranslate"><span class="pre">local_dataset</span></code>
field in the container, and hence you can always read data from <code class="docutils literal notranslate"><span class="pre">self.local_dataset</span></code></p></li>
<li><p>Alternatively, you can use the <code class="docutils literal notranslate"><span class="pre">prepare_data</span></code> method of a <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> to download data from the web,
for example. In this case, you don‚Äôt need to define any of the <code class="docutils literal notranslate"><span class="pre">local_dataset</span></code> or <code class="docutils literal notranslate"><span class="pre">azure_dataset_id</span></code> fields.</p></li>
</ul>
<p>In the above example, training is done for 42 epochs. After the model is trained, it will be evaluated on the test set,
via PyTorch Lightning‚Äôs <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html?highlight=trainer.test#test">built-in test functionality</a>.
See below for an alternative way of running the evaluation on the test set.</p>
</section>
<section id="data-loaders">
<h3>Data loaders<a class="headerlink" href="#data-loaders" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>The example above creates <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> objects from a dataset. When creating those, you need to specify a batch size
(how many samples from your dataset will go into one minibatch), and a number of worker processes. Note that, by
default, data loading will happen in the main process, meaning that your GPU will sit idle while the CPU reads data
from disk. When specifying a number of workers, it will spawn processes that pre-fetch data from disk, and put them
into a queue, ready for the GPU to pick it up when it is done processing the current minibatch.</p>
<p>For more details, please see the documentation for
<a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a>. There is also a
<a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">tutorial describing the foundations of datasets and
data loaders</a></p>
</section>
<section id="outputting-files-during-training">
<h3>Outputting files during training<a class="headerlink" href="#outputting-files-during-training" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>The Lightning model returned by <code class="docutils literal notranslate"><span class="pre">create_model</span></code> needs to write its output files to the current working directory.
When running the InnerEye toolbox outside of AzureML, the toolbox will change the current working directory to a
newly created output folder, with a name that contains the time stamp and and the model name.
When running the InnerEye toolbox in AzureML, the folder structure will be set up such that all files written
to the current working directory are later uploaded to Azure blob storage at the end of the AzureML job. The files
will also be later available via the AzureML UI.</p>
</section>
<section id="trainer-arguments">
<h3>Trainer arguments<a class="headerlink" href="#trainer-arguments" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>All arguments that control the PyTorch Lightning <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> object are defined in the class <code class="docutils literal notranslate"><span class="pre">TrainerParams</span></code>. A
<code class="docutils literal notranslate"><span class="pre">LightningContainer</span></code> object inherits from this class. The most essential one is the <code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> field, which controls
the <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> argument of the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>.</p>
<p>Usage example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">LightningModule</span><span class="p">,</span> <span class="n">LightningDataModule</span>
<span class="kn">from</span> <span class="nn">InnerEye.ML.lightning_container</span> <span class="kn">import</span> <span class="n">LightningContainer</span>
<span class="k">class</span> <span class="nc">MyContainer</span><span class="p">(</span><span class="n">LightningContainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">42</span>

    <span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LightningModule</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">MyLightningModel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_data_module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LightningDataModule</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">MyDataModule</span><span class="p">(</span><span class="n">root_path</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>For further details how the <code class="docutils literal notranslate"><span class="pre">TrainerParams</span></code> are used, refer to the <code class="docutils literal notranslate"><span class="pre">create_lightning_trainer</span></code> method in
<a class="reference external" href="https://github.com/microsoft/InnerEye-DeepLearning/tree/main/InnerEye/ML/model_training.py">InnerEye/ML/model_training.py</a></p>
</section>
<section id="optimizer-and-lr-scheduler-arguments">
<h3>Optimizer and LR scheduler arguments<a class="headerlink" href="#optimizer-and-lr-scheduler-arguments" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>There are two possible ways of choosing the optimizer and LR scheduler:</p>
<ul class="simple">
<li><p>The Lightning model returned by <code class="docutils literal notranslate"><span class="pre">create_model</span></code> can define its own <code class="docutils literal notranslate"><span class="pre">configure_optimizers</span></code> method, with the same
signature as <code class="docutils literal notranslate"><span class="pre">LightningModule.configure_optimizers</span></code>. This is the typical way of configuring it for Lightning models.</p></li>
<li><p>Alternatively, the model can inherit from <code class="docutils literal notranslate"><span class="pre">LightningModuleWithOptimizer</span></code>. This class implements a
<code class="docutils literal notranslate"><span class="pre">configure_optimizers</span></code> method that uses settings defined in the <code class="docutils literal notranslate"><span class="pre">OptimizerParams</span></code> class. These settings are all
available from the command line, and you can, for example, start a new run with a different learning rate by
supplying the additional commandline flag <code class="docutils literal notranslate"><span class="pre">--l_rate=1e-2</span></code>.</p></li>
</ul>
</section>
<section id="evaluating-the-trained-model">
<h3>Evaluating the trained model<a class="headerlink" href="#evaluating-the-trained-model" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>The InnerEye toolbox provides two possible routes of implementing that:</p>
<p>You can either use PyTorch Lightning‚Äôs built-in capabilities, via the <code class="docutils literal notranslate"><span class="pre">test_step</span></code> method. If the model that is
returned by <code class="docutils literal notranslate"><span class="pre">create_model</span></code> implements the <code class="docutils literal notranslate"><span class="pre">test_step</span></code> method, the InnerEye toolbox will use the <code class="docutils literal notranslate"><span class="pre">trainer.test</span></code> method
(see <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html?highlight=trainer.test#test">docs</a>).
In this case, the best checkpoint during training will be used. The test data is read via the data loader created
by the <code class="docutils literal notranslate"><span class="pre">test_dataloader</span></code> of the <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> that is used for training/validation.</p>
<p>Alternatively, the model can implement the methods defined in <code class="docutils literal notranslate"><span class="pre">InnerEyeInference</span></code>. In this case, the methods will be
call in this order:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">on_inference_start</span><span class="p">()</span>
<span class="k">for</span> <span class="n">dataset_split</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Train</span><span class="p">,</span> <span class="n">Val</span><span class="p">,</span> <span class="n">Test</span><span class="p">]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">on_inference_epoch_start</span><span class="p">(</span><span class="n">dataset_split</span><span class="p">,</span> <span class="n">is_ensemble_model</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">[</span><span class="n">dataset_split</span><span class="p">])):</span>
        <span class="n">model_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">inference_step</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">model_outputs</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">on_inference_epoch_end</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">on_inference_end</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="overriding-properties-on-the-commandline">
<h2>Overriding properties on the commandline<a class="headerlink" href="#overriding-properties-on-the-commandline" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>You can define hyperparameters that affect data and/or model, as in the following code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">param</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">LightningModule</span>
<span class="kn">from</span> <span class="nn">InnerEye.ML.lightning_container</span> <span class="kn">import</span> <span class="n">LightningContainer</span>
<span class="k">class</span> <span class="nc">DummyContainerWithParameters</span><span class="p">(</span><span class="n">LightningContainer</span><span class="p">):</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">Integer</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LightningModule</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">MyLightningModel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>All parameters added in this form will be automatically accessible from the commandline, there is no need to define
a separate argument parser: When starting training, you can add a flag like <code class="docutils literal notranslate"><span class="pre">--num_layers=7</span></code>.</p>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">ÔÉÅ</a></h2>
<section id="setting-only-the-required-fields">
<h3>Setting only the required fields<a class="headerlink" href="#setting-only-the-required-fields" title="Permalink to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">LightningModule</span><span class="p">,</span> <span class="n">LightningDataModule</span>
<span class="kn">from</span> <span class="nn">InnerEye.ML.lightning_container</span> <span class="kn">import</span> <span class="n">LightningContainer</span>

<span class="k">class</span> <span class="nc">Container1</span><span class="p">(</span><span class="n">LightningContainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">azure_dataset_id</span> <span class="o">=</span> <span class="s2">&quot;some_folder_in_azure&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">20</span>

    <span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LightningModule</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">MyLightningModel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_data_module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LightningDataModule</span><span class="p">:</span>
        <span class="c1"># This should read data from self.local_dataset. Before training, the data folder &quot;some_folder_in_azure&quot;</span>
        <span class="c1"># (given by self.azure_dataset_id) will be downloaded or mounted, and its local path set in</span>
        <span class="c1"># self.local_dataset</span>
        <span class="k">return</span> <span class="n">MyDataModule</span><span class="p">(</span><span class="n">root_folder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="adding-additional-arguments-for-the-pytorch-lightning-trainer">
<h3>Adding additional arguments for the PyTorch Lightning trainer<a class="headerlink" href="#adding-additional-arguments-for-the-pytorch-lightning-trainer" title="Permalink to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">LightningModule</span><span class="p">,</span> <span class="n">LightningDataModule</span>
<span class="kn">from</span> <span class="nn">InnerEye.ML.lightning_container</span> <span class="kn">import</span> <span class="n">LightningContainer</span>
<span class="k">class</span> <span class="nc">Container2</span><span class="p">(</span><span class="n">LightningContainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">azure_dataset_id</span> <span class="o">=</span> <span class="s2">&quot;some_folder_in_azure&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">20</span>

    <span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LightningModule</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">MyLightningModel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_data_module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LightningDataModule</span><span class="p">:</span>
        <span class="c1"># This should read data from self.local_dataset. Before training, the data folder &quot;some_folder_in_azure&quot;</span>
        <span class="c1"># (given by self.azure_dataset_id) will be downloaded or mounted, and its local path set in</span>
        <span class="c1"># self.local_dataset</span>
        <span class="k">return</span> <span class="n">MyDataModule</span><span class="p">(</span><span class="n">root_folder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_dataset</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_trainer_arguments</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="c1"># These arguments will be passed through to the Lightning trainer.</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;gradient_clip_val&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;limit_train_batches&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="fastmri.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Working with FastMRI models</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="deploy_on_aml.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Model Deployment</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; Microsoft Corporation
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Bring Your Own PyTorch Lightning Model</a><ul>
<li><a class="reference internal" href="#setup">Setup</a><ul>
<li><a class="reference internal" href="#cross-validation">Cross Validation</a></li>
<li><a class="reference internal" href="#data-loaders">Data loaders</a></li>
<li><a class="reference internal" href="#outputting-files-during-training">Outputting files during training</a></li>
<li><a class="reference internal" href="#trainer-arguments">Trainer arguments</a></li>
<li><a class="reference internal" href="#optimizer-and-lr-scheduler-arguments">Optimizer and LR scheduler arguments</a></li>
<li><a class="reference internal" href="#evaluating-the-trained-model">Evaluating the trained model</a></li>
</ul>
</li>
<li><a class="reference internal" href="#overriding-properties-on-the-commandline">Overriding properties on the commandline</a></li>
<li><a class="reference internal" href="#examples">Examples</a><ul>
<li><a class="reference internal" href="#setting-only-the-required-fields">Setting only the required fields</a></li>
<li><a class="reference internal" href="#adding-additional-arguments-for-the-pytorch-lightning-trainer">Adding additional arguments for the PyTorch Lightning trainer</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/scripts/furo.js"></script>
    </body>
</html>