<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Debugging and Monitoring Jobs" href="debugging_and_monitoring.html" /><link rel="prev" title="Building Models" href="building_models.html" />

    <meta name="generator" content="sphinx-5.0.2, furo 2022.06.21"/>
        <title>Sample Tasks - InnerEye-DeepLearning 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?digest=40978830699223671f4072448e654b5958f38b89" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">InnerEye-DeepLearning 1.0.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">InnerEye-DeepLearning 1.0.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="innereye_deeplearning.html">InnerEye-DeepLearning</a></li>
<li class="toctree-l1"><a class="reference internal" href="WSL.html">How to use the Windows Subsystem for Linux (WSL2) for development</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment.html">Set up InnerEye-DeepLearning</a></li>
<li class="toctree-l1"><a class="reference internal" href="setting_up_aml.html">How to setup Azure Machine Learning for InnerEye</a></li>
<li class="toctree-l1"><a class="reference internal" href="creating_dataset.html">Dataset Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="building_models.html">Building Models</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Sample Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="debugging_and_monitoring.html">Debugging and Monitoring Jobs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Further reading for contributors</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="pull_requests.html">Suggested Workflow for Pull Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Pytest and testing on CPU and GPU machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hello_world_model.html">Training a Hello World segmentation model</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_on_aml.html">Model Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="bring_your_own_model.html">Bring Your Own PyTorch Lightning Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="fastmri.html">Working with FastMRI models</a></li>
<li class="toctree-l1"><a class="reference internal" href="innereye_as_submodule.html">Using the InnerEye code as a git submodule of your project</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_diagnostics.html">Model Diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="move_model.html">Move a model to other workspace</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="self_supervised_models.html">Training of self-supervised models</a></li>
<li class="toctree-l1"><a class="reference internal" href="CHANGELOG.html">Changelog</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API documentation (üöß Work In Progress üöß)</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../rst/api/ML/index.html">Machine learning</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/configs.html">Segmentation Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/runner.html">Runner</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/augmentations.html">Data augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/photometric_normalization.html">Photometric normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst/api/ML/pipelines.html">Pipelines</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="sample-tasks">
<h1>Sample Tasks<a class="headerlink" href="#sample-tasks" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>This document contains two sample tasks for the classification and segmentation pipelines.</p>
<p>The document will walk through the steps in <a class="reference internal" href="building_models.html"><span class="doc">Training Steps</span></a>, but with specific examples for each task.
Before trying to train these models, you should have followed steps to set up an <a class="reference internal" href="environment.html"><span class="doc">environment</span></a> and <a class="reference internal" href="setting_up_aml.html"><span class="doc">AzureML</span></a></p>
<section id="sample-classification-task-glaucoma-detection-on-oct-volumes">
<h2>Sample classification task: Glaucoma Detection on OCT volumes<a class="headerlink" href="#sample-classification-task-glaucoma-detection-on-oct-volumes" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>This example is based on the paper <a class="reference external" href="https://arxiv.org/pdf/1807.04855v3.pdf">A feature agnostic approach for glaucoma detection in OCT volumes</a>.</p>
<section id="downloading-and-preparing-the-glaucoma-dataset">
<h3>Downloading and preparing the glaucoma dataset<a class="headerlink" href="#downloading-and-preparing-the-glaucoma-dataset" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>The dataset is available <a class="reference external" href="https://zenodo.org/record/1481223#.Xs-ehzPiuM_">here</a> <sup><a class="reference external" href="#1">[1]</a></sup>.</p>
<p>After downloading and extracting the zip file, run the <a class="reference external" href="https://github.com/microsoft/InnerEye-DeepLearning/blob/main/InnerEye/Scripts/create_glaucoma_dataset_csv.py">create_glaucoma_dataset_csv.py</a>
script on the extracted folder.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python create_dataset_csv.py /path/to/extracted/folder
</pre></div>
</div>
<p>This will convert the dataset to csv form and create a file <code class="docutils literal notranslate"><span class="pre">dataset.csv</span></code>.</p>
<p>Finally, upload this folder (with the images and <code class="docutils literal notranslate"><span class="pre">dataset.csv</span></code>) to Azure Blob Storage. For details on creating a storage account,
see <a class="reference external" href="setting_up_aml.md#step-4-create-a-storage-account-for-your-datasets">Setting up AzureML</a>. The dataset should go
into a container called <code class="docutils literal notranslate"><span class="pre">datasets</span></code>, with a folder name of your choice (<code class="docutils literal notranslate"><span class="pre">name_of_your_dataset_on_azure</span></code> in the
description below).</p>
</section>
<section id="creating-the-glaucoma-model-configuration-and-starting-training">
<h3>Creating the glaucoma model configuration and starting training<a class="headerlink" href="#creating-the-glaucoma-model-configuration-and-starting-training" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>Next, you need to create a configuration file <code class="docutils literal notranslate"><span class="pre">InnerEye/ML/configs/MyGlaucoma.py</span></code>
which extends the GlaucomaPublic class like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">InnerEye.ML.configs.classification.GlaucomaPublic</span> <span class="kn">import</span> <span class="n">GlaucomaPublic</span>
<span class="k">class</span> <span class="nc">MyGlaucomaModel</span><span class="p">(</span><span class="n">GlaucomaPublic</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">azure_dataset_id</span><span class="o">=</span><span class="s2">&quot;name_of_your_dataset_on_azure&quot;</span>
</pre></div>
</div>
<p>The value for <code class="docutils literal notranslate"><span class="pre">self.azure_dataset_id</span></code> should match the dataset upload location, called
<code class="docutils literal notranslate"><span class="pre">name_of_your_dataset_on_azure</span></code> above.</p>
<p>Once that config is in place, you can start training in AzureML via</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python InnerEye/ML/runner.py --model<span class="o">=</span>MyGlaucomaModel --azureml
</pre></div>
</div>
<p>As an alternative to working with a fork of the repository, you can use InnerEye-DeepLearning via a submodule.
Please check <a class="reference internal" href="innereye_as_submodule.html"><span class="doc">here</span></a> for details.</p>
</section>
</section>
<section id="sample-segmentation-task-segmentation-of-lung-ct">
<h2>Sample segmentation task: Segmentation of Lung CT<a class="headerlink" href="#sample-segmentation-task-segmentation-of-lung-ct" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>This example is based on the <a class="reference external" href="https://wiki.cancerimagingarchive.net/display/Public/Lung+CT+Segmentation+Challenge+2017">Lung CT Segmentation Challenge 2017</a> <sup><a class="reference external" href="#2">[2]</a></sup>.</p>
<section id="downloading-and-preparing-the-lung-dataset">
<h3>Downloading and preparing the lung dataset<a class="headerlink" href="#downloading-and-preparing-the-lung-dataset" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>The dataset <sup>[[3]][#3](<a class="reference external" href="#4">4</a></sup> can be downloaded <a class="reference external" href="https://wiki.cancerimagingarchive.net/display/Public/Lung+CT+Segmentation+Challenge+2017#021ca3c9a0724b0d9df784f1699d35e2">here</a>.</p>
<p>You need to convert the dataset from DICOM-RT to NIFTI. Before this, place the downloaded dataset in another
parent folder, which we will call <code class="docutils literal notranslate"><span class="pre">datasets</span></code>. This file structure is expected by the conversion tool.</p>
<p>Next, use the
<a class="reference external" href="https://github.com/microsoft/InnerEye-createdataset">InnerEye-CreateDataset</a> commandline tools to create a
NIFTI dataset from the downloaded (DICOM) files.
After installing the tool, run</p>
<div class="highlight-batch notranslate"><div class="highlight"><pre><span></span>InnerEye.CreateDataset.Runner.exe dataset --datasetRootDirectory=<span class="p">&lt;</span>path to the &#39;datasets&#39; folder<span class="p">&gt;</span> --niftiDatasetDirectory=<span class="p">&lt;</span>output folder name for converted dataset<span class="p">&gt;</span> --dicomDatasetDirectory=<span class="p">&lt;</span>name of downloaded folder inside &#39;datasets&#39;<span class="p">&gt;</span> --geoNorm 1;1;3
</pre></div>
</div>
<p>Now, you should have another folder under <code class="docutils literal notranslate"><span class="pre">datasets</span></code> with the converted Nifti files.
The <code class="docutils literal notranslate"><span class="pre">geonorm</span></code> tag tells the tool to normalize the voxel sizes during conversion.</p>
<p>Finally, upload this folder (with the images and dataset.csv) to Azure Blob Storage. For details on creating a storage account,
see <a class="reference external" href="setting_up_aml.md#step-4-create-a-storage-account-for-your-datasets">Setting up AzureML</a>. All files should go
into a folder in the <code class="docutils literal notranslate"><span class="pre">datasets</span></code> container, for example <code class="docutils literal notranslate"><span class="pre">my_lung_dataset</span></code>. This folder name will need to go into the
<code class="docutils literal notranslate"><span class="pre">azure_dataset_id</span></code> field of the model configuration, see below.</p>
</section>
<section id="creating-the-lung-model-configuration-and-starting-training">
<h3>Creating the lung model configuration and starting training<a class="headerlink" href="#creating-the-lung-model-configuration-and-starting-training" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>You can then create a new model configuration, based on the template
<a class="reference external" href="https://github.com/microsoft/InnerEye-DeepLearning/tree/main/InnerEye/ML/configs/segmentation/Lung.py">Lung.py</a>. To do this, create a file
<code class="docutils literal notranslate"><span class="pre">InnerEye/ML/configs/segmentation/MyLungModel.py</span></code>, where you create a subclass of the template Lung model, and
add the <code class="docutils literal notranslate"><span class="pre">azure_dataset_id</span></code> field (i.e., the name of the folder that contains the uploaded data from above),
so that it looks like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">InnerEye.ML.configs.segmentation.Lung</span> <span class="kn">import</span> <span class="n">Lung</span>
<span class="k">class</span> <span class="nc">MyLungModel</span><span class="p">(</span><span class="n">Lung</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">azure_dataset_id</span> <span class="o">=</span> <span class="s2">&quot;my_lung_dataset&quot;</span>
</pre></div>
</div>
<p>If you are using InnerEye as a submodule, please add this configuration in your private configuration folder,
as described for the Glaucoma model <a class="reference internal" href="innereye_as_submodule.html"><span class="doc">here</span></a>.</p>
<p>You can now run the following command to start a job on AzureML:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python InnerEye/ML/runner.py --azureml --model<span class="o">=</span>MyLungModel
</pre></div>
</div>
<p>See <a class="reference internal" href="building_models.html"><span class="doc">Model Training</span></a> for details on training outputs, resuming training, testing models and model ensembles.</p>
</section>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this heading">ÔÉÅ</a></h3>
<p><a id="1">[1]</a>
Ishikawa, Hiroshi. (2018). OCT volumes for glaucoma detection (Version 1.0.0) [Data set]. Zenodo. <a class="reference external" href="http://doi.org/10.5281/zenodo.1481223">http://doi.org/10.5281/zenodo.1481223</a></p>
<p><a id="2">[2]</a>
Yang, J. , Veeraraghavan, H. , Armato, S. G., Farahani, K. , Kirby, J. S., Kalpathy-Kramer, J. , van Elmpt, W. , Dekker, A. , Han, X. , Feng, X. , Aljabar, P. , Oliveira, B. , van der Heyden, B. , Zamdborg, L. , Lam, D. , Gooding, M. and Sharp, G. C. (2018),
Autosegmentation for thoracic radiation treatment planning: A grand challenge at AAPM 2017. Med. Phys.. . <a class="reference external" href="https://doi.org/10.1002/mp.13141">doi:10.1002/mp.13141</a></p>
<p><a id="3">[3]</a>
Yang, Jinzhong; Sharp, Greg; Veeraraghavan, Harini ; van Elmpt, Wouter ; Dekker, Andre; Lustberg, Tim; Gooding, Mark. (2017).
Data from Lung CT Segmentation Challenge. The Cancer Imaging Archive. <a class="reference external" href="http://doi.org/10.7937/K9/TCIA.2017.3r3fvz08">http://doi.org/10.7937/K9/TCIA.2017.3r3fvz08</a></p>
<p><a id="4">[4]</a>
Clark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle M, Tarbox L, Prior F.
The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository, Journal of Digital Imaging, Volume 26, Number 6, December, 2013, pp 1045-1057. (<a class="reference external" href="http://link.springer.com/article/10.1007%2Fs10278-013-9622-7">paper</a>)</p>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="debugging_and_monitoring.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Debugging and Monitoring Jobs</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="building_models.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Building Models</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; Microsoft Corporation
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Sample Tasks</a><ul>
<li><a class="reference internal" href="#sample-classification-task-glaucoma-detection-on-oct-volumes">Sample classification task: Glaucoma Detection on OCT volumes</a><ul>
<li><a class="reference internal" href="#downloading-and-preparing-the-glaucoma-dataset">Downloading and preparing the glaucoma dataset</a></li>
<li><a class="reference internal" href="#creating-the-glaucoma-model-configuration-and-starting-training">Creating the glaucoma model configuration and starting training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sample-segmentation-task-segmentation-of-lung-ct">Sample segmentation task: Segmentation of Lung CT</a><ul>
<li><a class="reference internal" href="#downloading-and-preparing-the-lung-dataset">Downloading and preparing the lung dataset</a></li>
<li><a class="reference internal" href="#creating-the-lung-model-configuration-and-starting-training">Creating the lung model configuration and starting training</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/scripts/furo.js"></script>
    </body>
</html>